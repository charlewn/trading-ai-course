{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 6: Analyzing Stock Sentiment from Twits\n",
    "## Instructions\n",
    "Each problem consists of a function to implement and instructions on how to implement the function.  The parts of the function that need to be implemented are marked with a `# TODO` comment.\n",
    "\n",
    "## Packages\n",
    "When you implement the functions, you'll only need to you use the packages you've used in the classroom, like [Pandas](https://pandas.pydata.org/) and [Numpy](http://www.numpy.org/). These packages will be imported for you. We recommend you don't add any import statements, otherwise the grader might not be able to run your code.\n",
    "\n",
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "When deciding the value of a company, it's important to follow the news. For example, a product recall or natural disaster in a company's product chain. You want to be able to turn this information into a signal. Currently, the best tool for the job is a Neural Network. \n",
    "\n",
    "For this project, you'll use posts from the social media site [StockTwits](https://en.wikipedia.org/wiki/StockTwits). The community on StockTwits is full of investors, traders, and entrepreneurs. Each message posted is called a Twit. This is similar to Twitter's version of a post, called a Tweet. You'll build a model around these twits that generate a sentiment score.\n",
    "\n",
    "We've collected a bunch of twits, then hand labeled the sentiment of each. To capture the degree of sentiment, we'll use a five-point scale: very negative, negative, neutral, positive, very positive. Each twit is labeled -2 to 2 in steps of 1, from very negative to very positive respectively. You'll build a sentiment analysis model that will learn to assign sentiment to twits on its own, using this labeled data.\n",
    "\n",
    "The first thing we should to do, is load the data.\n",
    "\n",
    "## Import Twits \n",
    "### Load Twits Data \n",
    "This JSON file contains a list of objects for each twit in the `'data'` field:\n",
    "\n",
    "```\n",
    "{'data':\n",
    "  {'message_body': 'Neutral twit body text here',\n",
    "   'sentiment': 0},\n",
    "  {'message_body': 'Happy twit body text here',\n",
    "   'sentiment': 1},\n",
    "   ...\n",
    "}\n",
    "```\n",
    "\n",
    "The fields represent the following:\n",
    "\n",
    "* `'message_body'`: The text of the twit.\n",
    "* `'sentiment'`: Sentiment score for the twit, ranges from -2 to 2 in steps of 1, with 0 being neutral.\n",
    "\n",
    "\n",
    "To see what the data look like by printing the first 10 twits from the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'message_body': '$FITB great buy at 26.00...ill wait', 'sentiment': 2, 'timestamp': '2018-07-01T00:00:09Z'}, {'message_body': '@StockTwits $MSFT', 'sentiment': 1, 'timestamp': '2018-07-01T00:00:42Z'}, {'message_body': '#STAAnalystAlert for $TDG : Jefferies Maintains with a rating of Hold setting target price at USD 350.00. Our own verdict is Buy  http://www.stocktargetadvisor.com/toprating', 'sentiment': 2, 'timestamp': '2018-07-01T00:01:24Z'}, {'message_body': '$AMD I heard thereâ€™s a guy who knows someone who thinks somebody knows something - on StockTwits.', 'sentiment': 1, 'timestamp': '2018-07-01T00:01:47Z'}, {'message_body': '$AMD reveal yourself!', 'sentiment': 0, 'timestamp': '2018-07-01T00:02:13Z'}, {'message_body': '$AAPL Why the drop? I warren Buffet taking out his position?', 'sentiment': 1, 'timestamp': '2018-07-01T00:03:10Z'}, {'message_body': '$BA bears have 1 reason on 06-29 to pay more attention https://dividendbot.com?s=BA', 'sentiment': -2, 'timestamp': '2018-07-01T00:04:09Z'}, {'message_body': '$BAC ok good we&#39;re not dropping in price over the weekend, lol', 'sentiment': 1, 'timestamp': '2018-07-01T00:04:17Z'}, {'message_body': '$AMAT - Daily Chart, we need to get back to above 50.', 'sentiment': 2, 'timestamp': '2018-07-01T00:08:01Z'}, {'message_body': '$GME 3% drop per week after spike... if no news in 3 months, back to 12s... if BO, then bingo... what is the odds?', 'sentiment': -2, 'timestamp': '2018-07-01T00:09:03Z'}]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('..', '..', 'data', 'project_6_stocktwits', 'twits.json'), 'r') as f:\n",
    "    twits = json.load(f)\n",
    "\n",
    "print(twits['data'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Data\n",
    "Now let's look at the number of twits in dataset. Print the number of twits below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1548010"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"print out the number of twits\"\"\"\n",
    "\n",
    "# TODO Implement \n",
    "len(twits[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Message Body and Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [twit['message_body'] for twit in twits['data']]\n",
    "# Since the sentiment scores are discrete, we'll scale the sentiments to 0 to 4 for use in our network\n",
    "sentiments = [twit['sentiment'] + 2 for twit in twits['data']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data\n",
    "With our data in hand we need to preprocess our text. These twits are collected by filtering on ticker symbols where these are denoted with a leader $ symbol in the twit itself. For example,\n",
    "\n",
    "`{'message_body': 'RT @google Our annual look at the year in Google blogging (and beyond) http://t.co/sptHOAh8 $GOOG',\n",
    " 'sentiment': 0}`\n",
    "\n",
    "The ticker symbols don't provide information on the sentiment, and they are in every twit, so we should remove them. This twit also has the `@google` username, again not providing sentiment information, so we should also remove it. We also see a URL `http://t.co/sptHOAh8`. Let's remove these too.\n",
    "\n",
    "The easiest way to remove specific words or phrases is with regex using the `re` module. You can sub out specific patterns with a space:\n",
    "\n",
    "```python\n",
    "re.sub(pattern, ' ', text)\n",
    "```\n",
    "This will substitute a space with anywhere the pattern matches in the text. Later when we tokenize the text, we'll split appropriately on those spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    This function takes a string as input, then performs these operations: \n",
    "        - lowercase\n",
    "        - remove URLs\n",
    "        - remove ticker symbols \n",
    "        - removes punctuation\n",
    "        - tokenize by splitting the string on whitespace \n",
    "        - removes any single character tokens\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        message : The text message to be preprocessed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        tokens: The preprocessed text into tokens.\n",
    "    \"\"\" \n",
    "    #TODO: Implement \n",
    "    \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    \n",
    "    # Replace URLs with a space in the message\n",
    "    pattern = r'[https]+://[a-zA-Z0-9/.//]+'\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    \n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    pattern = r'[$@][a-zA-Z0-9/.//@]+'\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    \n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    #text = pass see above\n",
    "\n",
    "    # Replace everything not a letter with a space\n",
    "    pattern = r'[^a-zA-Z]'\n",
    "    text = re.sub(pattern, \" \", text)\n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace into a list of words\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lemmatize words using the WordNetLemmatizer. You can ignore any word that is not longer than one character.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(token) for token in tokens]\n",
    "\n",
    "    \n",
    "    assert type(tokens) == list, 'Tokens should be list'\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: You must ensure that after preprocessing the text should NOT include:\n",
    "- Numbers\n",
    "- URLs\n",
    "- Single character tokens\n",
    "- Ticker symbols (these should be removed even if they don't appear at the beginning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess All the Twits \n",
    "Now we can preprocess each of the twits in our dataset. Apply the function `preprocess` to all the twit messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement\n",
    "\n",
    "tokenized = [preprocess(m) for m in messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "Now with all of our messages tokenized, we want to create a vocabulary and count up how often each word appears in our entire corpus. Use the [`Counter`](https://docs.python.org/3.1/library/collections.html#collections.Counter) function to count up all the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create a vocabulary by using Bag of words\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Implement \n",
    "\n",
    "#a = []\n",
    "\n",
    "#bow = [Counter(t) for t in tokenized]\n",
    "#bow\n",
    "bow = Counter()\n",
    "for t in tokenized:\n",
    "    bow.update(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Words Appearing in Message\n",
    "With our vocabulary, now we'll remove some of the most common words such as 'the', 'and', 'it', etc. These words don't contribute to identifying sentiment and are really common, resulting in a lot of noise in our input. If we can filter these out, then our network should have an easier time learning.\n",
    "\n",
    "We also want to remove really rare words that show up in a only a few twits. Here you'll want to divide the count of each word by the **number of messages** calculated in the code block above (i.e. `len(messages))`. Then remove words that only appear in some small fraction of the messages.\n",
    "\n",
    ">Note: There is not an exact number for low and high-frequency cut-offs, however there is a correct optimal range.\n",
    "You should ideally set up low-frequency cut-off from 0.0000002 to 0.000007 (inclusive) and high-frequency from 5 to 20 (inclusive). If the number is too big, we lose lots of important words that we can use in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 406439), ('to', 396290), ('amp', 393108), ('a', 359517), ('utm', 305643), ('is', 288779), ('for', 282643), ('on', 247585), ('s', 239873), ('of', 218912), ('and', 215163), ('in', 213484), ('this', 204525), ('it', 196667), ('i', 183626), ('at', 140819), ('will', 129397), ('up', 122995), ('source', 111588), ('are', 102729)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17420"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    freqs\n",
    "    low_cutoff\n",
    "    high_cutoff\n",
    "    K_most_common\n",
    "\"\"\"\n",
    "\n",
    "# TODO Implement \n",
    "\n",
    "# Dictionart that contains the Frequency of words appearing in messages.\n",
    "# The key is the token and the value is the frequency of that word in the corpus.\n",
    "total_freqs = sum([val for key, val in bow.items()])\n",
    "freqs = {key: val/total_freqs for key, val in bow.items()}\n",
    "\n",
    "# Float that is the frequency cutoff. Drop words with a frequency that is lower or equal to this number.\n",
    "low_cutoff = 0.0000007\n",
    "\n",
    "# Integer that is the cut off for most common words. Drop words that are the `high_cutoff` most common words.\n",
    "high_cutoff = 20\n",
    "\n",
    "# The k most common words in the corpus. Use `high_cutoff` as the k.\n",
    "K_most_common = bow.most_common(n=high_cutoff)\n",
    "\n",
    "\n",
    "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]\n",
    "print(K_most_common)\n",
    "len(filtered_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Vocabulary by Removing Filtered Words\n",
    "Let's creat three variables that will help with our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    vocab\n",
    "    id2vocab\n",
    "    filtered\n",
    "\"\"\"\n",
    "\n",
    "#TODO Implement\n",
    "\n",
    "# A dictionary for the `filtered_words`. The key is the word and value is an id that represents the word. \n",
    "vocab = {filtered_words[i]:i for i in range(0, len(filtered_words))}\n",
    "# Reverse of the `vocab` dictionary. The key is word id and value is the word. \n",
    "id2vocab = {v:k for k,v in vocab.items()}\n",
    "# tokenized with the words not in `filtered_words` removed.\n",
    "filtered = [[word for word in token if word in vocab] for token in tokenized]\n",
    "\n",
    "assert set(vocab.keys()) == set(id2vocab.values()), 'Check vocab and id2vocab dictionaries'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the classes\n",
    "Let's do a few last pre-processing steps. If we look at how our twits are labeled, we'll find that 50% of them are neutral. This means that our network will be 50% accurate just by guessing 0 every single time. To help our network learn appropriately, we'll want to balance our classes.\n",
    "That is, make sure each of our different sentiment scores show up roughly as frequently in the data.\n",
    "\n",
    "What we can do here is go through each of our examples and randomly drop twits with neutral sentiment. What should be the probability we drop these twits if we want to get around 20% neutral twits starting at 50% neutral? We should also take this opportunity to remove messages with length 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced = {'messages': [], 'sentiments':[]}\n",
    "\n",
    "n_neutral = sum(1 for each in sentiments if each == 2)\n",
    "N_examples = len(sentiments)\n",
    "keep_prob = (N_examples - n_neutral)/4/n_neutral\n",
    "\n",
    "for idx, sentiment in enumerate(sentiments):\n",
    "    message = filtered[idx]\n",
    "    if len(message) == 0:\n",
    "        # skip this message because it has length zero\n",
    "        continue\n",
    "    elif sentiment != 2 or random.random() < keep_prob:\n",
    "        balanced['messages'].append(message)\n",
    "        balanced['sentiments'].append(sentiment) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did it correctly, you should see the following result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19607725782598454"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
    "N_examples = len(balanced['sentiments'])\n",
    "n_neutral/N_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's convert our tokens into integer ids which we can pass to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [[vocab[word] for word in message] for message in balanced['messages']]\n",
    "sentiments = balanced['sentiments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "Now we have our vocabulary which means we can transform our tokens into ids, which are then passed to our network. So, let's define the network now!\n",
    "\n",
    "Here is a nice diagram showing the network we'd like to build: \n",
    "\n",
    "#### Embed -> RNN -> Dense -> Softmax\n",
    "### Implement the text classifier\n",
    "Before we build text classifier, if you remember from the other network that you built in  \"Sentiment Analysis with an RNN\"  exercise  - which there, the network called \" SentimentRNN\", here we named it \"TextClassifer\" - consists of three main parts: 1) init function `__init__` 2) forward pass `forward`  3) hidden state `init_hidden`. \n",
    "\n",
    "This network is pretty similar to the network you built expect in the  `forward` pass, we use softmax instead of sigmoid. The reason we are not using sigmoid is that the output of NN is not a binary. In our network, sentiment scores have 5 possible outcomes. We are looking for an outcome with the highest probability thus softmax is a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            vocab_size : The vocabulary size.\n",
    "            embed_size : The embedding layer size.\n",
    "            lstm_size : The LSTM layer size.\n",
    "            output_size : The output size.\n",
    "            lstm_layers : The number of LSTM layers.\n",
    "            dropout : The dropout probability.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        \n",
    "        # TODO Implement\n",
    "\n",
    "        # Setup embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # Setup additional layers\n",
    "        self.lstm = nn.LSTM(embed_size,lstm_size,lstm_layers,dropout=dropout,batch_first=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" \n",
    "        Initializes hidden state\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            batch_size : The size of batches.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            hidden_state\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement \n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        #print(torch.cuda.is_available())\n",
    "        #if (torch.cuda.is_available()):\n",
    "        #    hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_().cuda(),\n",
    "        #          weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_().cuda())\n",
    "        #else:\n",
    "        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "                      weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "\n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on nn_input.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            nn_input : The batch of input to the NN.\n",
    "            hidden_state : The LSTM hidden state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            logps: log softmax output\n",
    "            hidden_state: The new hidden state.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement \n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        \n",
    "        x = nn_input.long()\n",
    "        \n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
    "\n",
    "        lstm_out = lstm_out[-1,:,:] # getting the last time step output\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        out = self.softmax(out)\n",
    "        # return last sigmoid output and hidden state\n",
    "        return out, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4988, -1.3849, -1.9527, -1.7442, -1.5626],\n",
      "        [-1.4728, -1.3706, -1.9249, -1.7953, -1.5856],\n",
      "        [-1.4577, -1.3743, -1.9334, -1.8200, -1.5724],\n",
      "        [-1.4783, -1.3828, -1.9518, -1.7431, -1.5891]])\n"
     ]
    }
   ],
   "source": [
    "model = TextClassifier(len(vocab), 10, 6, 5, dropout=0.1, lstm_layers=2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "input = torch.randint(0, 1000, (5, 4), dtype=torch.int64)\n",
    "hidden = model.init_hidden(4)\n",
    "\n",
    "logps, _ = model.forward(input, hidden)\n",
    "print(logps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### DataLoaders and Batching\n",
    "Now we should build a generator that we can use to loop through our data. It'll be more efficient if we can pass our sequences in as batches. Our input tensors should look like `(sequence_length, batch_size)`. So if our sequences are 40 tokens long and we pass in 25 sequences, then we'd have an input size of `(40, 25)`.\n",
    "\n",
    "If we set our sequence length to 40, what do we do with messages that are more or less than 40 tokens? For messages with fewer than 40 tokens, we will pad the empty spots with zeros. We should be sure to **left** pad so that the RNN starts from nothing before going through the data. If the message has 20 tokens, then the first 20 spots of our 40 long sequence will be 0. If a message has more than 40 tokens, we'll just keep the first 40 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(messages, labels, sequence_length=30, batch_size=32, shuffle=False):\n",
    "    \"\"\" \n",
    "    Build a dataloader.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        indices = list(range(len(messages)))\n",
    "        random.shuffle(indices)\n",
    "        messages = [messages[idx] for idx in indices]\n",
    "        labels = [labels[idx] for idx in indices]\n",
    "\n",
    "    total_sequences = len(messages)\n",
    "\n",
    "    for ii in range(0, total_sequences, batch_size):\n",
    "        batch_messages = messages[ii: ii+batch_size]\n",
    "        \n",
    "        # First initialize a tensor of all zeros\n",
    "        batch = torch.zeros((sequence_length, len(batch_messages)), dtype=torch.int64)\n",
    "        for batch_num, tokens in enumerate(batch_messages):\n",
    "            token_tensor = torch.tensor(tokens)\n",
    "            # Left pad!\n",
    "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
    "            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n",
    "        \n",
    "        label_tensor = torch.tensor(labels[ii: ii+len(batch_messages)])\n",
    "        \n",
    "        yield batch, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and  Validation\n",
    "With our data in nice shape, we'll split it into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "828726"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Split data into training and validation datasets. Use an appropriate split size.\n",
    "The features are the `token_ids` and the labels are the `sentiments`.\n",
    "\"\"\"   \n",
    "\n",
    "# TODO Implement \n",
    "split= round(len(token_ids)*.8) # 3 to 1\n",
    "#print(split)\n",
    "#print(sentiments)\n",
    "\n",
    "train_features = token_ids[:split]\n",
    "valid_features = token_ids[split:]\n",
    "train_labels = sentiments[:split]\n",
    "valid_labels = sentiments[split:]\n",
    "\n",
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch, labels = next(iter(dataloader(train_features, train_labels, sequence_length=20, batch_size=64)))\n",
    "#model = TextClassifier(len(vocab), 200, 128, 5, dropout=0.)\n",
    "#hidden = model.init_hidden(64)\n",
    "#logps, hidden = model.forward(text_batch, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "It's time to train the neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (embedding): Embedding(17420, 512)\n",
       "  (lstm): LSTM(512, 128, num_layers=2, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2)\n",
       "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TextClassifier(len(vocab), 512, 128, 5, lstm_layers=2, dropout=0.2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch: 1 / 1 \tStep: 100 \n",
      "  Train Loss: 1.321   Valid Loss: 1.132   Valid Accuracies: 0.571\n",
      "Epoch: 1 / 1 \tStep: 200 \n",
      "  Train Loss: 1.211   Valid Loss: 1.215   Valid Accuracies: 0.357\n",
      "Epoch: 1 / 1 \tStep: 300 \n",
      "  Train Loss: 1.124   Valid Loss: 0.906   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 400 \n",
      "  Train Loss: 1.180   Valid Loss: 1.165   Valid Accuracies: 0.500\n",
      "Epoch: 1 / 1 \tStep: 500 \n",
      "  Train Loss: 1.136   Valid Loss: 1.309   Valid Accuracies: 0.500\n",
      "Epoch: 1 / 1 \tStep: 600 \n",
      "  Train Loss: 1.051   Valid Loss: 0.909   Valid Accuracies: 0.571\n",
      "Epoch: 1 / 1 \tStep: 700 \n",
      "  Train Loss: 0.953   Valid Loss: 1.140   Valid Accuracies: 0.571\n",
      "Epoch: 1 / 1 \tStep: 800 \n",
      "  Train Loss: 0.909   Valid Loss: 1.072   Valid Accuracies: 0.500\n",
      "Epoch: 1 / 1 \tStep: 900 \n",
      "  Train Loss: 0.980   Valid Loss: 1.210   Valid Accuracies: 0.357\n",
      "Epoch: 1 / 1 \tStep: 1000 \n",
      "  Train Loss: 0.883   Valid Loss: 1.231   Valid Accuracies: 0.571\n",
      "Epoch: 1 / 1 \tStep: 1100 \n",
      "  Train Loss: 0.772   Valid Loss: 1.132   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 1200 \n",
      "  Train Loss: 0.857   Valid Loss: 1.237   Valid Accuracies: 0.500\n",
      "Epoch: 1 / 1 \tStep: 1300 \n",
      "  Train Loss: 0.999   Valid Loss: 0.827   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 1400 \n",
      "  Train Loss: 1.004   Valid Loss: 1.173   Valid Accuracies: 0.357\n",
      "Epoch: 1 / 1 \tStep: 1500 \n",
      "  Train Loss: 0.694   Valid Loss: 0.768   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 1600 \n",
      "  Train Loss: 0.994   Valid Loss: 1.258   Valid Accuracies: 0.500\n",
      "Epoch: 1 / 1 \tStep: 1700 \n",
      "  Train Loss: 0.857   Valid Loss: 0.766   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 1800 \n",
      "  Train Loss: 0.756   Valid Loss: 1.068   Valid Accuracies: 0.500\n",
      "Epoch: 1 / 1 \tStep: 1900 \n",
      "  Train Loss: 1.012   Valid Loss: 0.797   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 2000 \n",
      "  Train Loss: 1.018   Valid Loss: 0.945   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 2100 \n",
      "  Train Loss: 0.811   Valid Loss: 0.625   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 2200 \n",
      "  Train Loss: 0.766   Valid Loss: 0.688   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 2300 \n",
      "  Train Loss: 0.857   Valid Loss: 0.704   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 2400 \n",
      "  Train Loss: 0.970   Valid Loss: 0.976   Valid Accuracies: 0.429\n",
      "Epoch: 1 / 1 \tStep: 2500 \n",
      "  Train Loss: 0.933   Valid Loss: 0.508   Valid Accuracies: 0.857\n",
      "Epoch: 1 / 1 \tStep: 2600 \n",
      "  Train Loss: 0.749   Valid Loss: 1.135   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 2700 \n",
      "  Train Loss: 0.851   Valid Loss: 0.761   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 2800 \n",
      "  Train Loss: 0.761   Valid Loss: 1.030   Valid Accuracies: 0.571\n",
      "Epoch: 1 / 1 \tStep: 2900 \n",
      "  Train Loss: 0.653   Valid Loss: 1.260   Valid Accuracies: 0.500\n",
      "Epoch: 1 / 1 \tStep: 3000 \n",
      "  Train Loss: 0.858   Valid Loss: 0.907   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 3100 \n",
      "  Train Loss: 0.702   Valid Loss: 0.954   Valid Accuracies: 0.500\n",
      "Epoch: 1 / 1 \tStep: 3200 \n",
      "  Train Loss: 0.907   Valid Loss: 1.425   Valid Accuracies: 0.500\n",
      "Epoch: 1 / 1 \tStep: 3300 \n",
      "  Train Loss: 0.612   Valid Loss: 0.829   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 3400 \n",
      "  Train Loss: 0.794   Valid Loss: 0.864   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 3500 \n",
      "  Train Loss: 0.961   Valid Loss: 0.883   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 3600 \n",
      "  Train Loss: 0.714   Valid Loss: 1.020   Valid Accuracies: 0.500\n",
      "Epoch: 1 / 1 \tStep: 3700 \n",
      "  Train Loss: 0.873   Valid Loss: 0.915   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 3800 \n",
      "  Train Loss: 0.877   Valid Loss: 0.666   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 3900 \n",
      "  Train Loss: 0.652   Valid Loss: 0.868   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 4000 \n",
      "  Train Loss: 0.671   Valid Loss: 1.102   Valid Accuracies: 0.571\n",
      "Epoch: 1 / 1 \tStep: 4100 \n",
      "  Train Loss: 0.775   Valid Loss: 0.662   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 4200 \n",
      "  Train Loss: 0.743   Valid Loss: 0.453   Valid Accuracies: 1.000\n",
      "Epoch: 1 / 1 \tStep: 4300 \n",
      "  Train Loss: 0.662   Valid Loss: 0.519   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 4400 \n",
      "  Train Loss: 0.895   Valid Loss: 0.947   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 4500 \n",
      "  Train Loss: 0.611   Valid Loss: 0.942   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 4600 \n",
      "  Train Loss: 0.888   Valid Loss: 0.738   Valid Accuracies: 0.571\n",
      "Epoch: 1 / 1 \tStep: 4700 \n",
      "  Train Loss: 0.793   Valid Loss: 0.785   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 4800 \n",
      "  Train Loss: 0.738   Valid Loss: 0.589   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 4900 \n",
      "  Train Loss: 0.607   Valid Loss: 0.947   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 5000 \n",
      "  Train Loss: 0.860   Valid Loss: 0.715   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 5100 \n",
      "  Train Loss: 0.729   Valid Loss: 0.972   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 5200 \n",
      "  Train Loss: 0.837   Valid Loss: 0.860   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 5300 \n",
      "  Train Loss: 0.859   Valid Loss: 0.986   Valid Accuracies: 0.571\n",
      "Epoch: 1 / 1 \tStep: 5400 \n",
      "  Train Loss: 0.751   Valid Loss: 0.668   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 5500 \n",
      "  Train Loss: 0.815   Valid Loss: 1.037   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 5600 \n",
      "  Train Loss: 0.693   Valid Loss: 0.641   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 5700 \n",
      "  Train Loss: 0.818   Valid Loss: 1.169   Valid Accuracies: 0.500\n",
      "Epoch: 1 / 1 \tStep: 5800 \n",
      "  Train Loss: 0.760   Valid Loss: 1.177   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 5900 \n",
      "  Train Loss: 0.965   Valid Loss: 0.818   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 6000 \n",
      "  Train Loss: 0.672   Valid Loss: 0.938   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 6100 \n",
      "  Train Loss: 0.759   Valid Loss: 0.718   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 6200 \n",
      "  Train Loss: 0.840   Valid Loss: 0.936   Valid Accuracies: 0.500\n",
      "Epoch: 1 / 1 \tStep: 6300 \n",
      "  Train Loss: 1.010   Valid Loss: 0.846   Valid Accuracies: 0.571\n",
      "Epoch: 1 / 1 \tStep: 6400 \n",
      "  Train Loss: 0.785   Valid Loss: 1.031   Valid Accuracies: 0.571\n",
      "Epoch: 1 / 1 \tStep: 6500 \n",
      "  Train Loss: 0.807   Valid Loss: 1.097   Valid Accuracies: 0.500\n",
      "Epoch: 1 / 1 \tStep: 6600 \n",
      "  Train Loss: 0.694   Valid Loss: 1.106   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 6700 \n",
      "  Train Loss: 0.771   Valid Loss: 0.799   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 6800 \n",
      "  Train Loss: 0.821   Valid Loss: 0.856   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 6900 \n",
      "  Train Loss: 0.931   Valid Loss: 0.549   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 7000 \n",
      "  Train Loss: 0.724   Valid Loss: 0.389   Valid Accuracies: 0.857\n",
      "Epoch: 1 / 1 \tStep: 7100 \n",
      "  Train Loss: 0.759   Valid Loss: 0.827   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 7200 \n",
      "  Train Loss: 0.532   Valid Loss: 0.920   Valid Accuracies: 0.571\n",
      "Epoch: 1 / 1 \tStep: 7300 \n",
      "  Train Loss: 0.810   Valid Loss: 0.603   Valid Accuracies: 0.857\n",
      "Epoch: 1 / 1 \tStep: 7400 \n",
      "  Train Loss: 0.800   Valid Loss: 0.790   Valid Accuracies: 0.571\n",
      "Epoch: 1 / 1 \tStep: 7500 \n",
      "  Train Loss: 0.933   Valid Loss: 0.771   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 7600 \n",
      "  Train Loss: 0.730   Valid Loss: 0.783   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 7700 \n",
      "  Train Loss: 0.598   Valid Loss: 0.567   Valid Accuracies: 0.857\n",
      "Epoch: 1 / 1 \tStep: 7800 \n",
      "  Train Loss: 0.697   Valid Loss: 0.438   Valid Accuracies: 0.857\n",
      "Epoch: 1 / 1 \tStep: 7900 \n",
      "  Train Loss: 0.746   Valid Loss: 0.828   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 8000 \n",
      "  Train Loss: 0.711   Valid Loss: 0.840   Valid Accuracies: 0.500\n",
      "Epoch: 1 / 1 \tStep: 8100 \n",
      "  Train Loss: 0.761   Valid Loss: 0.808   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 8200 \n",
      "  Train Loss: 0.547   Valid Loss: 0.660   Valid Accuracies: 0.857\n",
      "Epoch: 1 / 1 \tStep: 8300 \n",
      "  Train Loss: 0.764   Valid Loss: 0.529   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 8400 \n",
      "  Train Loss: 0.703   Valid Loss: 1.316   Valid Accuracies: 0.429\n",
      "Epoch: 1 / 1 \tStep: 8500 \n",
      "  Train Loss: 0.881   Valid Loss: 0.494   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 8600 \n",
      "  Train Loss: 0.706   Valid Loss: 1.359   Valid Accuracies: 0.429\n",
      "Epoch: 1 / 1 \tStep: 8700 \n",
      "  Train Loss: 0.741   Valid Loss: 1.131   Valid Accuracies: 0.571\n",
      "Epoch: 1 / 1 \tStep: 8800 \n",
      "  Train Loss: 0.680   Valid Loss: 0.740   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 8900 \n",
      "  Train Loss: 0.637   Valid Loss: 0.305   Valid Accuracies: 0.929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 1 \tStep: 9000 \n",
      "  Train Loss: 0.678   Valid Loss: 0.660   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 9100 \n",
      "  Train Loss: 0.636   Valid Loss: 0.697   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 9200 \n",
      "  Train Loss: 0.742   Valid Loss: 0.615   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 9300 \n",
      "  Train Loss: 0.787   Valid Loss: 0.578   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 9400 \n",
      "  Train Loss: 0.952   Valid Loss: 0.766   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 9500 \n",
      "  Train Loss: 0.801   Valid Loss: 0.931   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 9600 \n",
      "  Train Loss: 0.619   Valid Loss: 1.029   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 9700 \n",
      "  Train Loss: 0.777   Valid Loss: 0.780   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 9800 \n",
      "  Train Loss: 0.820   Valid Loss: 0.676   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 9900 \n",
      "  Train Loss: 0.594   Valid Loss: 0.377   Valid Accuracies: 0.929\n",
      "Epoch: 1 / 1 \tStep: 10000 \n",
      "  Train Loss: 0.779   Valid Loss: 1.381   Valid Accuracies: 0.500\n",
      "Epoch: 1 / 1 \tStep: 10100 \n",
      "  Train Loss: 0.743   Valid Loss: 0.985   Valid Accuracies: 0.500\n",
      "Epoch: 1 / 1 \tStep: 10200 \n",
      "  Train Loss: 0.775   Valid Loss: 0.437   Valid Accuracies: 0.929\n",
      "Epoch: 1 / 1 \tStep: 10300 \n",
      "  Train Loss: 0.500   Valid Loss: 0.651   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 10400 \n",
      "  Train Loss: 0.790   Valid Loss: 0.975   Valid Accuracies: 0.500\n",
      "Epoch: 1 / 1 \tStep: 10500 \n",
      "  Train Loss: 0.871   Valid Loss: 0.619   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 10600 \n",
      "  Train Loss: 0.670   Valid Loss: 1.179   Valid Accuracies: 0.429\n",
      "Epoch: 1 / 1 \tStep: 10700 \n",
      "  Train Loss: 0.698   Valid Loss: 0.724   Valid Accuracies: 0.857\n",
      "Epoch: 1 / 1 \tStep: 10800 \n",
      "  Train Loss: 0.653   Valid Loss: 0.734   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 10900 \n",
      "  Train Loss: 0.573   Valid Loss: 0.856   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 11000 \n",
      "  Train Loss: 0.714   Valid Loss: 0.311   Valid Accuracies: 0.857\n",
      "Epoch: 1 / 1 \tStep: 11100 \n",
      "  Train Loss: 0.803   Valid Loss: 0.612   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 11200 \n",
      "  Train Loss: 0.661   Valid Loss: 0.726   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 11300 \n",
      "  Train Loss: 0.619   Valid Loss: 0.907   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 11400 \n",
      "  Train Loss: 0.750   Valid Loss: 0.934   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 11500 \n",
      "  Train Loss: 0.610   Valid Loss: 0.888   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 11600 \n",
      "  Train Loss: 0.884   Valid Loss: 0.312   Valid Accuracies: 0.857\n",
      "Epoch: 1 / 1 \tStep: 11700 \n",
      "  Train Loss: 0.642   Valid Loss: 1.022   Valid Accuracies: 0.571\n",
      "Epoch: 1 / 1 \tStep: 11800 \n",
      "  Train Loss: 0.781   Valid Loss: 0.948   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 11900 \n",
      "  Train Loss: 0.991   Valid Loss: 0.828   Valid Accuracies: 0.571\n",
      "Epoch: 1 / 1 \tStep: 12000 \n",
      "  Train Loss: 0.577   Valid Loss: 0.411   Valid Accuracies: 0.857\n",
      "Epoch: 1 / 1 \tStep: 12100 \n",
      "  Train Loss: 0.735   Valid Loss: 0.496   Valid Accuracies: 0.857\n",
      "Epoch: 1 / 1 \tStep: 12200 \n",
      "  Train Loss: 0.677   Valid Loss: 0.993   Valid Accuracies: 0.571\n",
      "Epoch: 1 / 1 \tStep: 12300 \n",
      "  Train Loss: 0.693   Valid Loss: 0.309   Valid Accuracies: 0.857\n",
      "Epoch: 1 / 1 \tStep: 12400 \n",
      "  Train Loss: 0.608   Valid Loss: 0.591   Valid Accuracies: 0.929\n",
      "Epoch: 1 / 1 \tStep: 12500 \n",
      "  Train Loss: 0.606   Valid Loss: 0.620   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 12600 \n",
      "  Train Loss: 0.806   Valid Loss: 0.986   Valid Accuracies: 0.643\n",
      "Epoch: 1 / 1 \tStep: 12700 \n",
      "  Train Loss: 0.650   Valid Loss: 0.503   Valid Accuracies: 0.786\n",
      "Epoch: 1 / 1 \tStep: 12800 \n",
      "  Train Loss: 0.788   Valid Loss: 0.511   Valid Accuracies: 0.714\n",
      "Epoch: 1 / 1 \tStep: 12900 \n",
      "  Train Loss: 0.746   Valid Loss: 0.749   Valid Accuracies: 0.786\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train your model with dropout. Make sure to clip your gradients.\n",
    "Print the training loss, validation loss, and validation accuracy for every 100 steps.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# help from https://knowledge.udacity.com/questions/37107\n",
    "# most information are extracted from the udacity help forum.\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "clip = 5\n",
    "print_every = 100\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch {}'.format(epoch + 1))\n",
    "    \n",
    "    steps = 0\n",
    "    # def dataloader(messages, labels, sequence_length=30, batch_size=32, shuffle=False):\n",
    "    for text_batch, labels in dataloader(\n",
    "            train_features, train_labels, batch_size=batch_size, sequence_length=20, shuffle=True):\n",
    "        steps += 1\n",
    "        # hidden = pass\n",
    "        # labels = tensor 64\n",
    "        hidden = model.init_hidden(labels.shape[0])\n",
    "        \n",
    "        #if steps == 100:\n",
    "        #    break\n",
    "        \n",
    "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each=each.to(device)\n",
    "        \n",
    "        model.zero_grad() \n",
    "        \n",
    "        hidden = tuple([each.data for each in hidden]) # detaching\n",
    "        \n",
    "        logps, hidden = model.forward(text_batch, hidden)        \n",
    "        \n",
    "        loss = criterion(logps.squeeze(), labels)\n",
    "        \n",
    "        loss.backward()  \n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        \n",
    "        optimizer.step()\n",
    "        # TODO Implement: Train Model\n",
    "        \n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            \n",
    "            train_losses = []\n",
    "            valid_losses = []\n",
    "            valid_accuracies = []\n",
    "            # TODO Implement: Print metrics\n",
    "            for text_batch, labels in dataloader(valid_features, valid_labels,\n",
    "                                                batch_size=batch_size, sequence_length=20,\n",
    "                                                shuffle=True):\n",
    "\n",
    "                valid_hidden = model.init_hidden(labels.shape[0])      \n",
    "                \n",
    "                text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "                for each in valid_hidden:\n",
    "                    each=each.to(device)\n",
    " \n",
    "                valid_log_probs, valid_hidden = model.forward(text_batch, valid_hidden)\n",
    "                valid_loss = criterion(valid_log_probs, labels)\n",
    "                                \n",
    "                # Accuracy\n",
    "                probs = torch.exp(valid_log_probs)\n",
    "                top_prob, top_class = probs.topk(1)\n",
    "                equality = top_class == labels.view(*top_class.shape)\n",
    "                valid_accuracy = torch.mean(equality.type(torch.FloatTensor))\n",
    "                                \n",
    "            train_losses.append(loss.item())\n",
    "            valid_losses.append(valid_loss.item())\n",
    "            valid_accuracies.append(valid_accuracy.item())\n",
    "            \n",
    "            model.train()\n",
    "            # Print result progress...            \n",
    "            print(f'Epoch: {epoch+1} / {epochs} \\tStep: {steps}',\n",
    "                  f'\\n  Train Loss: {loss.item():.3f}',\n",
    "                  f'  Valid Loss: {valid_loss.item():.3f}',\n",
    "                  f'  Valid Accuracies: {valid_accuracy.item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "### Prediction \n",
    "Okay, now that you have a trained model, try it on some new twits and see if it works appropriately. Remember that for any new text, you'll need to preprocess it first before passing it to the network. Implement the `predict` function to generate the prediction vector from a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, vocab):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : The string to make a prediction on.\n",
    "        model : The model to use for making the prediction.\n",
    "        vocab : Dictionary for word to word ids. The key is the word and the value is the word id.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pred : Prediction vector\n",
    "    \"\"\"    \n",
    "    \n",
    "    # TODO Implement\n",
    "    \n",
    "    tokens = preprocess(text)\n",
    "    \n",
    "    # Filter non-vocab words\n",
    "    tokens = [word for word in tokens if word in vocab]\n",
    "    # Convert words to ids\n",
    "    tokens = [vocab[word] for word in tokens if word in vocab]\n",
    "        \n",
    "    # Adding a batch dimension\n",
    "    text_input = torch.tensor(tokens).view(-1,1)\n",
    "    # Get the NN output\n",
    "    hidden = model.init_hidden(1)\n",
    "    logps, _ = model(text_input, hidden)\n",
    "    # Take the exponent of the NN output to get a range of 0 to 1 for each label.\n",
    "    pred = logps.exp()\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0010,  0.0209,  0.0118,  0.8629,  0.1035]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Google is working on self driving cars, I'm bullish on $goog\"\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "predict(text, model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: What is the prediction of the model? What is the uncertainty of the prediction?\n",
    "** TODO: Answer Question**\n",
    "\n",
    "The tensor([[ 0.0010,  0.0209,  0.0118,  0.8629,  0.1035]]) gives us 0.8629 for the forth item, meaning there is a high chance that the sentiment of this twit is positive. 0.0010 as the first item, it is highly unlikely this is a very negative twit.\n",
    "\n",
    "The uncertainty of the prediction lies in the accuracy of the model. First, some of the batches we have 0.786 accuracies, meaning we can expect a 78.6% accurate in our prediction. Second, our datasets does not include every positive and negative sentiment messages. It might not be able to give accurate predictions for data that it has not seen before. \n",
    "\n",
    "from https://knowledge.udacity.com/questions/530970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a trained model and we can make predictions. We can use this model to track the sentiments of various stocks by predicting the sentiments of twits as they are coming in. Now we have a stream of twits. For each of those twits, pull out the stocks mentioned in them and keep track of the sentiments. Remember that in the twits, ticker symbols are encoded with a dollar sign as the first character, all caps, and 2-4 letters, like $AAPL. Ideally, you'd want to track the sentiments of the stocks in your universe and use this as a signal in your larger model(s).\n",
    "\n",
    "## Testing\n",
    "### Load the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('..', '..', 'data', 'project_6_stocktwits', 'test_twits.json'), 'r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twit Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_body': '$JWN has moved -1.69% on 10-31. Check out the movement and peers at  https://dividendbot.com?s=JWN',\n",
       " 'timestamp': '2018-11-01T00:00:05Z'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def twit_stream():\n",
    "    for twit in test_data['data']:\n",
    "        yield twit\n",
    "\n",
    "next(twit_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `prediction` function, let's apply it to a stream of twits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_twits(stream, model, vocab, universe):\n",
    "    \"\"\" \n",
    "    Given a stream of twits and a universe of tickers, return sentiment scores for tickers in the universe.\n",
    "    \"\"\"\n",
    "    for twit in stream:\n",
    "\n",
    "        # Get the message text\n",
    "        text = twit['message_body']\n",
    "        symbols = re.findall('\\$[A-Z]{2,4}', text)\n",
    "        score = predict(text, model, vocab)\n",
    "\n",
    "        for symbol in symbols:\n",
    "            if symbol in universe:\n",
    "                yield {'symbol': symbol, 'score': score, 'timestamp': twit['timestamp']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'symbol': '$AAPL',\n",
       " 'score': tensor([[ 0.0672,  0.0977,  0.3137,  0.2785,  0.2430]]),\n",
       " 'timestamp': '2018-11-01T00:00:18Z'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "universe = {'$BBRY', '$AAPL', '$AMZN', '$BABA', '$YHOO', '$LQMT', '$FB', '$GOOG', '$BBBY', '$JNUG', '$SBUX', '$MU'}\n",
    "score_stream = score_twits(twit_stream(), model, vocab, universe)\n",
    "\n",
    "next(score_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. You have successfully built a model for sentiment analysis! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Now that you're done with the project, it's time to submit it. Click the submit button in the bottom right. One of our reviewers will give you feedback on your project with a pass or not passed grade. You can continue to the next section while you wait for feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
